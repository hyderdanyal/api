# -*- coding: utf-8 -*-
"""Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/welcome.ipynb
"""

# pip install scikit-surprise

import numpy as np
import pandas as pd
from scipy.sparse.linalg import svds
# Import libraries from Surprise package
from surprise import Reader, Dataset, SVD, accuracy
from surprise.model_selection import cross_validate,KFold
from RecommendResponse import *
import json

def Recommendation(uid):
    # Reading ratings file
    ratings = pd.read_csv('dataset/ratings.csv',  usecols=['user_id', 'book_id', 'rating'],encoding="ISO-8859-1")
    ratings['user_id'] = ratings['user_id'].apply(str)
    # Reading users file
    # users = pd.read_csv('dataset/users.csv', sep='\t', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'],encoding="ISO-8859-1")

    # Reading movies file
    books = pd.read_csv('dataset/newbooks.csv',  usecols=['book_id','title', 'genre','image_url','desc'],encoding="ISO-8859-1")

    # books.head()

    # ratings.head()

    # Top 20 movies that User 1310 has rated 
    # already_rated.head(20)

    # predictions

    # Load Reader library
    # reader = Reader()

    # # Load ratings dataset with Dataset library
    # data = Dataset.load_from_df(ratings[['user_id', 'book_id', 'rating']], reader)

    # # Split the dataset for 5-fold evaluation
    # kf = KFold(n_splits=5)
    # svd = SVD()

    # for trainset, testset in kf.split(data):

    #     # train and test algorithm.
    #     svd.fit(trainset)
    #     predictions = svd.test(testset)

    #     # Compute and print Root Mean Squared Error
    #     accuracy.rmse(predictions, verbose=True)

    # trainset = data.build_full_trainset()
    # svd.fit(trainset)

    # predictions

    # a= ratings[ratings['user_id'] == 1310]
    # pd.set_option("display.max_rows", None, "display.max_columns", None)
    # a

    # svd.predict(1310,26)
    import pickle

    with open('model_pickle','rb') as f:
        mp=pickle.load(f)


    # uid=1310
    # ratings
    # for i in ratings.user_id:
    h=[]
    
    print(type(uid),'******************************************************',type(ratings.user_id[0]))
    print('aaaaaaaaaaa',ratings.head(10))
    c=ratings[ratings['user_id'] == uid]
    print('ddddddd',c)
    d=books[~books.book_id.isin(c.book_id)]
    # print(d)
    # print(len(c),len(d),len(books))
    for _,row in d.iterrows():
        h.append(mp.predict(uid,row.book_id))
    h=pd.DataFrame(h)
    sortedest=h.sort_values(by='est',ascending=False).head(10)
    # sortedest

    mergedest=d.merge(sortedest[['est']],left_on='book_id',right_on=sortedest.iid)

    # Recommendation=books[books.book_id.isin(sortedest.iid)]
    Recommendation= mergedest.sort_values(by='est',ascending=False)
    # Recommendation
    print('Recommended Books for ',uid)
    print(Recommendation)
    titles=Recommendation['title']
    bk_id=Recommendation['book_id']
    url=Recommendation['image_url']
    des=Recommendation['desc']
    indices = pd.Series(Recommendation.index, index=Recommendation['title'])
    data=[]
    for i in indices:
        rec=RecommendResponse(titles.iloc[i],str(bk_id.iloc[i]),url.iloc[i],des.iloc[i])
        rec=json.dumps(rec.__dict__)
        rec=json.loads(rec)
        data.append(rec)
    
    return data    